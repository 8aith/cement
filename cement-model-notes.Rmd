Notes on cement modeling
========================================================

Here are the best regression models we've come up with:

```{r loaddata}
require(earth)
require(ggplot2)
source("cement-util.R")
master.table <- dget(file="cement-table.dat")
master.table <- master.table[master.table$year %% 5 == 1,]
testing.countries <- dget(file="testing-countries.dat")
set.seed(8675309)
#options(width=160)
```

Cluster with and without the modified population density.  Well use 4-5 clusters for the basic, 4-6 for the population density:

```{r docluster}
basic.predictors <- c("GDP.rate", "pcGDP", "urban.growth", "urban.pop", "pop.rate")
pden.predictors  <- c(basic.predictors, "pop.den")
## Lists of vars that include the output variable and the ISO code.  We need these to 
## filter the master table appropriately
basic.fullvars <- c(basic.predictors, "pcc.rate", "ISO")
basic.numvars  <- c(basic.predictors, "pcc.rate")
pden.fullvars  <- c(pden.predictors, "pcc.rate", "ISO")
pden.numvars   <- c(basic.predictors, "pcc.rate")

basic.table <- select.complete(master.table[,basic.fullvars])
basic.normalized <- clust.normalize(basic.table[,basic.predictors])
km.basic.4 <- kmeans(basic.normalized, centers=4)
km.basic.5 <- kmeans(basic.normalized, centers=5)

pden.table <- select.complete(master.table[,pden.fullvars])
pden.normalized <- clust.normalize(pden.table[,pden.predictors])
km.pden.4 <- kmeans(pden.normalized, centers=4)
km.pden.5 <- kmeans(pden.normalized, centers=5)
km.pden.6 <- kmeans(pden.normalized, centers=6)
```

Show the boxplots for the clusters, to give us an idea of what they cover.  Note we include the output variable, even if we didn't cluster on it  Clusters with basic vars:

```{r clustplot.basic, fig.width=10}
cluster.boxplot(km.basic.4, basic.table, basic.numvars)
cluster.boxplot(km.basic.5, basic.table, basic.numvars)
```

Clusters including (modified) population density:
```{r clustplot.pden, fig.width=10, fig.height=10}
cluster.boxplot(km.pden.4, pden.table, pden.numvars)
cluster.boxplot(km.pden.5, pden.table, pden.numvars)
cluster.boxplot(km.pden.6, pden.table, pden.numvars)
```

How similar are these clusters?  Check the similarity matrices
```{r simmatrix}
basic.members.4 <- clust.members(basic.table, km.basic.4$cluster)
basic.members.5 <- clust.members(basic.table, km.basic.5$cluster)
mlist.sim.matrix(basic.members.4, basic.members.4)
mlist.sim.matrix(basic.members.5, basic.members.5)
mlist.sim.matrix(basic.members.4, basic.members.5)

pden.members.4 <- clust.members(pden.table, km.pden.4$cluster)
pden.members.5 <- clust.members(pden.table, km.pden.5$cluster)
pden.members.6 <- clust.members(pden.table, km.pden.6$cluster)
mlist.sim.matrix(pden.members.4, pden.members.4)
mlist.sim.matrix(pden.members.5, pden.members.5)
mlist.sim.matrix(pden.members.6, pden.members.6)
mlist.sim.matrix(pden.members.4, pden.members.5)
mlist.sim.matrix(pden.members.4, pden.members.6)
mlist.sim.matrix(pden.members.5, pden.members.6)
```

Based on this it looks like the 6-cluster pden model might be kind of splitting cluster 5.1 into 6.3 and 6.4.  For the time being, I'm going to drop it.  (We need a better way of evaluating the clustering.  Maybe 5.1 _should_ have been split.  Look into measures of compactness.)

Comparing clusters with different sets of predictors is kind of tricky because the input datasets are (potentially) different.  For now we'll just do the modeling on each group of clusters separately and compare the results.

```{r fitmodels}
## same as basic.predictors and pden.predictors above, but expressed as a formula
basic.f <- pcc.rate~GDP.rate + pcGDP + urban.growth + urban.pop + pop.rate
pden.f  <- pcc.rate~pcGDP + GDP.rate + urban.growth + urban.pop + pop.rate + pop.den

## split basic data set into testing and training sets
basic.split <- master.table[complete.cases(master.table[,basic.fullvars]),]
basic.split <- split(basic.split, basic.split$ISO %in% testing.countries)
names(basic.split) <- c("training","testing")

## Fit the whole dataset with degre=1 and degree=2.  
basic.lm <- lm(formula=basic.f, data=basic.split$training)
basic.earth.d1 <- earth(formula=basic.f, data=basic.split$training, degree=1)
basic.earth.d2 <- earth(formula=basic.f, data=basic.split$training, degree=2)


## Evaluate RMS errors.  The rms.eval function wants a list of testing and training 
## datasets, which we don't really have (yet), so insert a dummy for the testing set
rms.eval(basic.lm, basic.split, prn=FALSE)
rms.eval(basic.earth.d1, basic.split, prn=FALSE)
rms.eval(basic.earth.d2, basic.split, prn=FALSE)
  
```

The earth model with the basic parameters and degree 1 seems to be our best contender so far.  Degree 2 does better on the training set, but worse on the training set, a clear indicator of overfitting.  Here's the scatterplot for that and for the basic linear model:

```{r scatterplot.earth.basic.d1}
scatterplot.model(basic.lm, basic.split, sz=2)
scatterplot.model(basic.earth.d1, basic.split, sz=2)
```

The cluster results are also intriguing. Cross-validation is complicated by the fact that the testing countries don't fall evenly across the clusters, so we've only done it in a limited way.

```{r cluster5}
cmodel.basic.5.lm <- cluster.modelfit(km.basic.5, basic.table, formula=basic.f, fitfun=lm, testset=testing.countries)
cmodel.basic.5.d1 <- cluster.modelfit(km.basic.5, basic.table, formula=basic.f, fitfun=earth, degree=1)
cmodel.basic.5.d1alt <- cluster.modelfit(km.basic.5, basic.table, formula=basic.f, fitfun=earth, testset=testing.countries, degree=1)
cmodel.basic.5.d2 <- cluster.modelfit(km.basic.5, basic.table, formula=basic.f, fitfun=earth, degree=2)
cmodel.pden.5.lm <- cluster.modelfit(km.pden.5, pden.table, formula=pden.f, fitfun=lm, testset=testing.countries)
cmodel.pden.5.d1 <- cluster.modelfit(km.pden.5, pden.table, formula=pden.f, fitfun=earth, degree=1)
cmodel.pden.5.d2 <- cluster.modelfit(km.pden.5, pden.table, formula=pden.f, fitfun=earth, degree=2)

cluster.rms.eval(km.basic.5, basic.table, cmodel.basic.5.lm, prn=FALSE, testing.set=testing.countries)
cluster.rms.eval(km.basic.5, basic.table, cmodel.basic.5.d1, prn=FALSE, testing.set=testing.countries)
cluster.rms.eval(km.basic.5, basic.table, cmodel.basic.5.d1alt, prn=FALSE, testing.set=testing.countries)
cluster.rms.eval(km.basic.5, basic.table, cmodel.basic.5.d2, prn=FALSE, testing.set=testing.countries)

cluster.rms.eval(km.pden.5, pden.table, cmodel.pden.5.lm, prn=FALSE, testing.set=testing.countries)
cluster.rms.eval(km.pden.5, pden.table, cmodel.pden.5.d1, prn=FALSE, testing.set=testing.countries)
cluster.rms.eval(km.pden.5, pden.table, cmodel.pden.5.d2, prn=FALSE, testing.set=testing.countries)
```

The degree 2 looks slightly better, though that's without any cross-validation applied.  Here are the scatterplots:

```{r cluster.scatterplot}
cluster.scatterplot.model(km.basic.5, basic.table, cmodel.basic.5.d2, sz=2)
cluster.scatterplot.model(km.pden.5, pden.table, cmodel.pden.5.d2, sz=2)
```

The degree 2 fits look a little suspect.  Clusters 2 and 4 are being fit mostly by the intercept term.

How do the models differ with and without the clustering (and with different versions of the clustering)?  Here are the model summaries for the straight linear model.

```{r model.summaries}
summary(basic.lm)
lapply(cmodel.basic.5.lm, summary)
#lapply(cmodel.basic.5.d1alt, summary)
lapply(cmodel.pden.5.lm, summary)
```
